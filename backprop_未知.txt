function [hidlw outlw terr] = backprop(tset, tslb, inihidlw, inioutlw, lr)
% derivative of sigmoid activation function
% tset - training set (every row represents a sample)
% tslb - column vector of labels 
% inihidlw - initial hidden layer weight matrix
% inioutlw - initial output layer weight matrix
% lr - learning rate

% hidlw - hidden layer weight matrix
% outlw - output layer weight matrix
% terr - total squared error of the ANN

% 1. Set output matrices to initial values 初始化权重
	hidlw = inihidlw;
	outlw = inioutlw;
	
% 2. Set total error to 0 初始化权重
	terr = 0;

% for each sample in the training set 遍历训练集中的每一个样本
	for i=1:rows(tset)

		% 3. Set desired output of the ANN (it depends on actf you use!)		
        %desire = zeros (1, columns(outlw)） - 1; %老师课上写的，chat说不对
		desire = -1 * ones(1, columns(outlw));
		desire(tslb(i)) = 1;

		% 4. Propagate input forward through the ANN
        input = [tset(i, :) 1];
		hlout = actf([input] * hidlw);
		olout = actf([hlout 1] * outlw);  % 隐层输出加偏置 -> 输出层

		% 5. Adjust total error
        terr += sumsq(desire - olout);
	
		% 6. Compute delta error of the output layer 这里的 actdf 输入必须是激活后的值 olout
        outdelta = (desire - olout) .* actdf(olout);

		% 7. Compute delta error of the hidden layer
        %hiddelta = (outlw(1:end-1,:) * outdelta')';%老师写的，chat说不对
		hiddelta = (outlw(1:end-1,:)* outdelta')' .* actdf(hlout);

		% 8. Update output layer weights
        outlw += lr * [hlout 1]' * outdelta;

		% 9. Update hidden layer weights
		%hidlw += lr * [input 1]' * hiddelta %老师写的，chat说不对老师写的，chat说不对
		hidlw += lr * input' * hiddelta;

	end


% function [hidlw, outlw, terr] = backprop(tset, tslb, inihidlw, inioutlw, lr)
%     % hidlw - 隐藏层权重矩阵
%     % outlw - 输出层权重矩阵
%     % terr - 总平方误差

%     % 1. 初始化权重
%     hidlw = inihidlw;
%     outlw = inioutlw;

%     % 2. 初始化总误差
%     terr = 0;

%     % 遍历训练集中的每一个样本
%     for i = 1:rows(tset)

%         % --- 目标向量设置 (修正) ---
%         % 假设 outlw 的列数等于输出层神经元数量（即类别数）
%         num_classes = columns(outlw); 
%         desire = -1 * ones(1, columns(outlw)); % 初始化全为 -1
        
%         % [修复1] 使用当前样本的标签 tslb(i)，而不是永远取第一个 tslb(1)
%         % 注意：确保 tslb 中的标签值范围是 1 到 num_classes
%         desire(tslb(i)) = 1;                   % 目标类别设为 1

%         % --- 前向传播 (修正) ---
        
%         % 输入层 -> 隐藏层
%         % [修复2] input 已经在末尾加了1 (Bias)，计算时直接相乘，不要再次加 [input 1]
%         input_with_bias = [tset(i, :) 1]; 
        
%         % 计算隐藏层加权和 (Weighted Sum)
%         hid_net = input_with_bias * hidlw;
%         % 激活函数输出
%         hlout = actf(hid_net);

%         % 隐藏层 -> 输出层
%         hlout_with_bias = [hlout 1]; % 为隐藏层输出添加偏置
%         out_net = hlout_with_bias * outlw;
%         olout = actf(out_net);

%         % --- 计算误差 ---
%         terr = terr + sumsq(desire - olout);

%         % --- 反向传播 (修正) ---

%         % 6. 计算输出层的 Delta (误差项)
%         % [修复3] 解决了 plout 未定义问题。
%         % 标准公式: Delta = (Target - Output) * f'(Output_Net)
%         % 假设 actdf 是激活函数的导数。如果使用 Sigmoid，导数通常可以用输出值计算：y*(1-y)
%         % 这里传入 olout 作为导数函数的参数
%         outdelta = (desire - olout) .* actdf(olout);

%         % 7. 计算隐藏层的 Delta
%         % [修复4] 原代码缺少乘以隐藏层激活函数的导数 actdf(hlout)
%         % 另外，需要去掉输出层权重矩阵中对应的偏置行 (最后一行)，因为它不反向传播到隐藏节点
%         weights_no_bias = outlw(1:end-1, :);
        
%         % 这里的 ' 表示转置，.* 表示对应元素相乘
%         hiddelta = (outdelta * weights_no_bias') .* actdf(hlout);

%         % --- 权重更新 (修正) ---

%         % 8. 更新输出层权重
%         % [修复5] 必须乘以学习率 lr
%         outlw = outlw + lr * (hlout_with_bias' * outdelta);

%         % 9. 更新隐藏层权重
%         hidlw = hidlw + lr * (input_with_bias' * hiddelta);

%     end
% end